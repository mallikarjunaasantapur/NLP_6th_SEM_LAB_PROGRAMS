{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMyged6fppWw"
      },
      "source": [
        "## Performing Operations lemming and stemming on text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y95ANb9ppWx"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTBtv1b5ppWx",
        "outputId": "0354092a-48c5-4683-cafb-34c9d05814a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natural language processing refers to the branch of\n",
            " computer science and more specifically the branch of \n",
            " artificial intelligence, concerned with giving computers\n",
            " the ability to understand text and spoken words in much \n",
            " the same way human beings can. Human language is filled with\n",
            " ambiguities that make it incredibly difficult to write software\n",
            " that accurately determines the intended meaning of text or voice\n",
            " data. Homonyms, homophones,sarcasm, idioms, metaphors, grammar\n",
            " and usage exceptions,variations in sentence structure. These\n",
            " just a few of the irregularities of human language that take\n",
            " humans years to learn, but that programmers must teach natural\n",
            " language driven applications to recognize and understand \n",
            " accurately from the start if those applications are going to be\n",
            " useful.\n"
          ]
        }
      ],
      "source": [
        "with open('../Rizzources/StemLem.txt') as f:\n",
        "    text = f.readlines()\n",
        "    text = \" \".join(text)\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vrs0_6x9ppWx",
        "outputId": "414fe667-4725-4734-b600-a76cd38cfa3c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/student/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "token = word_tokenize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCpmErauppWy"
      },
      "source": [
        "### PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VasLMLoppWy",
        "outputId": "50c34ae7-01b1-4381-af52-46bf9b8d794e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natural ----> natur\n",
            "language ----> languag\n",
            "processing ----> process\n",
            "refers ----> refer\n",
            "to ----> to\n",
            "the ----> the\n",
            "branch ----> branch\n",
            "of ----> of\n",
            "computer ----> comput\n",
            "science ----> scienc\n",
            "and ----> and\n",
            "more ----> more\n",
            "specifically ----> specif\n",
            "the ----> the\n",
            "branch ----> branch\n",
            "of ----> of\n",
            "artificial ----> artifici\n",
            "intelligence ----> intellig\n",
            ", ----> ,\n",
            "concerned ----> concern\n",
            "with ----> with\n",
            "giving ----> give\n",
            "computers ----> comput\n",
            "the ----> the\n",
            "ability ----> abil\n",
            "to ----> to\n",
            "understand ----> understand\n",
            "text ----> text\n",
            "and ----> and\n",
            "spoken ----> spoken\n",
            "words ----> word\n",
            "in ----> in\n",
            "much ----> much\n",
            "the ----> the\n",
            "same ----> same\n",
            "way ----> way\n",
            "human ----> human\n",
            "beings ----> be\n",
            "can ----> can\n",
            ". ----> .\n",
            "Human ----> human\n",
            "language ----> languag\n",
            "is ----> is\n",
            "filled ----> fill\n",
            "with ----> with\n",
            "ambiguities ----> ambigu\n",
            "that ----> that\n",
            "make ----> make\n",
            "it ----> it\n",
            "incredibly ----> incred\n",
            "difficult ----> difficult\n",
            "to ----> to\n",
            "write ----> write\n",
            "software ----> softwar\n",
            "that ----> that\n",
            "accurately ----> accur\n",
            "determines ----> determin\n",
            "the ----> the\n",
            "intended ----> intend\n",
            "meaning ----> mean\n",
            "of ----> of\n",
            "text ----> text\n",
            "or ----> or\n",
            "voice ----> voic\n",
            "data ----> data\n",
            ". ----> .\n",
            "Homonyms ----> homonym\n",
            ", ----> ,\n",
            "homophones ----> homophon\n",
            ", ----> ,\n",
            "sarcasm ----> sarcasm\n",
            ", ----> ,\n",
            "idioms ----> idiom\n",
            ", ----> ,\n",
            "metaphors ----> metaphor\n",
            ", ----> ,\n",
            "grammar ----> grammar\n",
            "and ----> and\n",
            "usage ----> usag\n",
            "exceptions ----> except\n",
            ", ----> ,\n",
            "variations ----> variat\n",
            "in ----> in\n",
            "sentence ----> sentenc\n",
            "structure ----> structur\n",
            ". ----> .\n",
            "These ----> these\n",
            "just ----> just\n",
            "a ----> a\n",
            "few ----> few\n",
            "of ----> of\n",
            "the ----> the\n",
            "irregularities ----> irregular\n",
            "of ----> of\n",
            "human ----> human\n",
            "language ----> languag\n",
            "that ----> that\n",
            "take ----> take\n",
            "humans ----> human\n",
            "years ----> year\n",
            "to ----> to\n",
            "learn ----> learn\n",
            ", ----> ,\n",
            "but ----> but\n",
            "that ----> that\n",
            "programmers ----> programm\n",
            "must ----> must\n",
            "teach ----> teach\n",
            "natural ----> natur\n",
            "language ----> languag\n",
            "driven ----> driven\n",
            "applications ----> applic\n",
            "to ----> to\n",
            "recognize ----> recogn\n",
            "and ----> and\n",
            "understand ----> understand\n",
            "accurately ----> accur\n",
            "from ----> from\n",
            "the ----> the\n",
            "start ----> start\n",
            "if ----> if\n",
            "those ----> those\n",
            "applications ----> applic\n",
            "are ----> are\n",
            "going ----> go\n",
            "to ----> to\n",
            "be ----> be\n",
            "useful ----> use\n",
            ". ----> .\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "for t in token:\n",
        "  print(t, '----> '+st.stem(t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vKF_-RcppWy"
      },
      "source": [
        "### Snowball Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8RiH7HAppWy",
        "outputId": "69f0c940-0708-4943-d3d3-4d85bf689a65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natural  ----> natur\n",
            "language  ----> languag\n",
            "processing  ----> process\n",
            "refers  ----> refer\n",
            "to  ----> to\n",
            "the  ----> the\n",
            "branch  ----> branch\n",
            "of  ----> of\n",
            "computer  ----> comput\n",
            "science  ----> scienc\n",
            "and  ----> and\n",
            "more  ----> more\n",
            "specifically  ----> specif\n",
            "the  ----> the\n",
            "branch  ----> branch\n",
            "of  ----> of\n",
            "artificial  ----> artifici\n",
            "intelligence  ----> intellig\n",
            ",  ----> ,\n",
            "concerned  ----> concern\n",
            "with  ----> with\n",
            "giving  ----> give\n",
            "computers  ----> comput\n",
            "the  ----> the\n",
            "ability  ----> abil\n",
            "to  ----> to\n",
            "understand  ----> understand\n",
            "text  ----> text\n",
            "and  ----> and\n",
            "spoken  ----> spoken\n",
            "words  ----> word\n",
            "in  ----> in\n",
            "much  ----> much\n",
            "the  ----> the\n",
            "same  ----> same\n",
            "way  ----> way\n",
            "human  ----> human\n",
            "beings  ----> be\n",
            "can  ----> can\n",
            ".  ----> .\n",
            "Human  ----> human\n",
            "language  ----> languag\n",
            "is  ----> is\n",
            "filled  ----> fill\n",
            "with  ----> with\n",
            "ambiguities  ----> ambigu\n",
            "that  ----> that\n",
            "make  ----> make\n",
            "it  ----> it\n",
            "incredibly  ----> incred\n",
            "difficult  ----> difficult\n",
            "to  ----> to\n",
            "write  ----> write\n",
            "software  ----> softwar\n",
            "that  ----> that\n",
            "accurately  ----> accur\n",
            "determines  ----> determin\n",
            "the  ----> the\n",
            "intended  ----> intend\n",
            "meaning  ----> mean\n",
            "of  ----> of\n",
            "text  ----> text\n",
            "or  ----> or\n",
            "voice  ----> voic\n",
            "data  ----> data\n",
            ".  ----> .\n",
            "Homonyms  ----> homonym\n",
            ",  ----> ,\n",
            "homophones  ----> homophon\n",
            ",  ----> ,\n",
            "sarcasm  ----> sarcasm\n",
            ",  ----> ,\n",
            "idioms  ----> idiom\n",
            ",  ----> ,\n",
            "metaphors  ----> metaphor\n",
            ",  ----> ,\n",
            "grammar  ----> grammar\n",
            "and  ----> and\n",
            "usage  ----> usag\n",
            "exceptions  ----> except\n",
            ",  ----> ,\n",
            "variations  ----> variat\n",
            "in  ----> in\n",
            "sentence  ----> sentenc\n",
            "structure  ----> structur\n",
            ".  ----> .\n",
            "These  ----> these\n",
            "just  ----> just\n",
            "a  ----> a\n",
            "few  ----> few\n",
            "of  ----> of\n",
            "the  ----> the\n",
            "irregularities  ----> irregular\n",
            "of  ----> of\n",
            "human  ----> human\n",
            "language  ----> languag\n",
            "that  ----> that\n",
            "take  ----> take\n",
            "humans  ----> human\n",
            "years  ----> year\n",
            "to  ----> to\n",
            "learn  ----> learn\n",
            ",  ----> ,\n",
            "but  ----> but\n",
            "that  ----> that\n",
            "programmers  ----> programm\n",
            "must  ----> must\n",
            "teach  ----> teach\n",
            "natural  ----> natur\n",
            "language  ----> languag\n",
            "driven  ----> driven\n",
            "applications  ----> applic\n",
            "to  ----> to\n",
            "recognize  ----> recogn\n",
            "and  ----> and\n",
            "understand  ----> understand\n",
            "accurately  ----> accur\n",
            "from  ----> from\n",
            "the  ----> the\n",
            "start  ----> start\n",
            "if  ----> if\n",
            "those  ----> those\n",
            "applications  ----> applic\n",
            "are  ----> are\n",
            "going  ----> go\n",
            "to  ----> to\n",
            "be  ----> be\n",
            "useful  ----> use\n",
            ".  ----> .\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "st = SnowballStemmer('english')\n",
        "for t in token:\n",
        "  print(t, ' ----> ' + st.stem(t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYC57Uf5ppWy"
      },
      "source": [
        "### Lancaster Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "im6efwkRppWy",
        "outputId": "b642380d-6902-485e-da06-eb5b63cae60c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natural  ----> nat\n",
            "language  ----> langu\n",
            "processing  ----> process\n",
            "refers  ----> ref\n",
            "to  ----> to\n",
            "the  ----> the\n",
            "branch  ----> branch\n",
            "of  ----> of\n",
            "computer  ----> comput\n",
            "science  ----> sci\n",
            "and  ----> and\n",
            "more  ----> mor\n",
            "specifically  ----> spec\n",
            "the  ----> the\n",
            "branch  ----> branch\n",
            "of  ----> of\n",
            "artificial  ----> art\n",
            "intelligence  ----> intellig\n",
            ",  ----> ,\n",
            "concerned  ----> concern\n",
            "with  ----> with\n",
            "giving  ----> giv\n",
            "computers  ----> comput\n",
            "the  ----> the\n",
            "ability  ----> abl\n",
            "to  ----> to\n",
            "understand  ----> understand\n",
            "text  ----> text\n",
            "and  ----> and\n",
            "spoken  ----> spok\n",
            "words  ----> word\n",
            "in  ----> in\n",
            "much  ----> much\n",
            "the  ----> the\n",
            "same  ----> sam\n",
            "way  ----> way\n",
            "human  ----> hum\n",
            "beings  ----> being\n",
            "can  ----> can\n",
            ".  ----> .\n",
            "Human  ----> hum\n",
            "language  ----> langu\n",
            "is  ----> is\n",
            "filled  ----> fil\n",
            "with  ----> with\n",
            "ambiguities  ----> ambigu\n",
            "that  ----> that\n",
            "make  ----> mak\n",
            "it  ----> it\n",
            "incredibly  ----> incred\n",
            "difficult  ----> difficult\n",
            "to  ----> to\n",
            "write  ----> writ\n",
            "software  ----> softw\n",
            "that  ----> that\n",
            "accurately  ----> acc\n",
            "determines  ----> determin\n",
            "the  ----> the\n",
            "intended  ----> intend\n",
            "meaning  ----> mean\n",
            "of  ----> of\n",
            "text  ----> text\n",
            "or  ----> or\n",
            "voice  ----> voic\n",
            "data  ----> dat\n",
            ".  ----> .\n",
            "Homonyms  ----> homonym\n",
            ",  ----> ,\n",
            "homophones  ----> homophon\n",
            ",  ----> ,\n",
            "sarcasm  ----> sarcasm\n",
            ",  ----> ,\n",
            "idioms  ----> idiom\n",
            ",  ----> ,\n",
            "metaphors  ----> metaph\n",
            ",  ----> ,\n",
            "grammar  ----> gramm\n",
            "and  ----> and\n",
            "usage  ----> us\n",
            "exceptions  ----> exceiv\n",
            ",  ----> ,\n",
            "variations  ----> vary\n",
            "in  ----> in\n",
            "sentence  ----> sent\n",
            "structure  ----> structure\n",
            ".  ----> .\n",
            "These  ----> thes\n",
            "just  ----> just\n",
            "a  ----> a\n",
            "few  ----> few\n",
            "of  ----> of\n",
            "the  ----> the\n",
            "irregularities  ----> irregul\n",
            "of  ----> of\n",
            "human  ----> hum\n",
            "language  ----> langu\n",
            "that  ----> that\n",
            "take  ----> tak\n",
            "humans  ----> hum\n",
            "years  ----> year\n",
            "to  ----> to\n",
            "learn  ----> learn\n",
            ",  ----> ,\n",
            "but  ----> but\n",
            "that  ----> that\n",
            "programmers  ----> program\n",
            "must  ----> must\n",
            "teach  ----> teach\n",
            "natural  ----> nat\n",
            "language  ----> langu\n",
            "driven  ----> driv\n",
            "applications  ----> apply\n",
            "to  ----> to\n",
            "recognize  ----> recogn\n",
            "and  ----> and\n",
            "understand  ----> understand\n",
            "accurately  ----> acc\n",
            "from  ----> from\n",
            "the  ----> the\n",
            "start  ----> start\n",
            "if  ----> if\n",
            "those  ----> thos\n",
            "applications  ----> apply\n",
            "are  ----> ar\n",
            "going  ----> going\n",
            "to  ----> to\n",
            "be  ----> be\n",
            "useful  ----> us\n",
            ".  ----> .\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "st = LancasterStemmer()\n",
        "for t in token:\n",
        "  print(t, ' ----> '+st.stem(t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8h9o7zlppWy"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axuDqjRIppWz"
      },
      "source": [
        "### Spacy Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuN1aZIUppWz",
        "outputId": "bd99c9b2-1427-4f1a-d532-a0322aa5ed31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natural  ----> natural\n",
            "language  ----> language\n",
            "processing  ----> processing\n",
            "refers  ----> refer\n",
            "to  ----> to\n",
            "the  ----> the\n",
            "branch  ----> branch\n",
            "of  ----> of\n",
            "\n",
            "   ----> \n",
            " \n",
            "computer  ----> computer\n",
            "science  ----> science\n",
            "and  ----> and\n",
            "more  ----> more\n",
            "specifically  ----> specifically\n",
            "the  ----> the\n",
            "branch  ----> branch\n",
            "of  ----> of\n",
            "\n",
            "   ----> \n",
            " \n",
            "artificial  ----> artificial\n",
            "intelligence  ----> intelligence\n",
            ",  ----> ,\n",
            "concerned  ----> concern\n",
            "with  ----> with\n",
            "giving  ----> give\n",
            "computers  ----> computer\n",
            "\n",
            "   ----> \n",
            " \n",
            "the  ----> the\n",
            "ability  ----> ability\n",
            "to  ----> to\n",
            "understand  ----> understand\n",
            "text  ----> text\n",
            "and  ----> and\n",
            "spoken  ----> spoken\n",
            "words  ----> word\n",
            "in  ----> in\n",
            "much  ----> much\n",
            "\n",
            "   ----> \n",
            " \n",
            "the  ----> the\n",
            "same  ----> same\n",
            "way  ----> way\n",
            "human  ----> human\n",
            "beings  ----> being\n",
            "can  ----> can\n",
            ".  ----> .\n",
            "Human  ----> human\n",
            "language  ----> language\n",
            "is  ----> be\n",
            "filled  ----> fill\n",
            "with  ----> with\n",
            "\n",
            "   ----> \n",
            " \n",
            "ambiguities  ----> ambiguity\n",
            "that  ----> that\n",
            "make  ----> make\n",
            "it  ----> it\n",
            "incredibly  ----> incredibly\n",
            "difficult  ----> difficult\n",
            "to  ----> to\n",
            "write  ----> write\n",
            "software  ----> software\n",
            "\n",
            "   ----> \n",
            " \n",
            "that  ----> that\n",
            "accurately  ----> accurately\n",
            "determines  ----> determine\n",
            "the  ----> the\n",
            "intended  ----> intend\n",
            "meaning  ----> meaning\n",
            "of  ----> of\n",
            "text  ----> text\n",
            "or  ----> or\n",
            "voice  ----> voice\n",
            "\n",
            "   ----> \n",
            " \n",
            "data  ----> datum\n",
            ".  ----> .\n",
            "Homonyms  ----> Homonyms\n",
            ",  ----> ,\n",
            "homophones  ----> homophone\n",
            ",  ----> ,\n",
            "sarcasm  ----> sarcasm\n",
            ",  ----> ,\n",
            "idioms  ----> idiom\n",
            ",  ----> ,\n",
            "metaphors  ----> metaphor\n",
            ",  ----> ,\n",
            "grammar  ----> grammar\n",
            "\n",
            "   ----> \n",
            " \n",
            "and  ----> and\n",
            "usage  ----> usage\n",
            "exceptions  ----> exception\n",
            ",  ----> ,\n",
            "variations  ----> variation\n",
            "in  ----> in\n",
            "sentence  ----> sentence\n",
            "structure  ----> structure\n",
            ".  ----> .\n",
            "These  ----> these\n",
            "\n",
            "   ----> \n",
            " \n",
            "just  ----> just\n",
            "a  ----> a\n",
            "few  ----> few\n",
            "of  ----> of\n",
            "the  ----> the\n",
            "irregularities  ----> irregularity\n",
            "of  ----> of\n",
            "human  ----> human\n",
            "language  ----> language\n",
            "that  ----> that\n",
            "take  ----> take\n",
            "\n",
            "   ----> \n",
            " \n",
            "humans  ----> human\n",
            "years  ----> year\n",
            "to  ----> to\n",
            "learn  ----> learn\n",
            ",  ----> ,\n",
            "but  ----> but\n",
            "that  ----> that\n",
            "programmers  ----> programmer\n",
            "must  ----> must\n",
            "teach  ----> teach\n",
            "natural  ----> natural\n",
            "\n",
            "   ----> \n",
            " \n",
            "language  ----> language\n",
            "driven  ----> drive\n",
            "applications  ----> application\n",
            "to  ----> to\n",
            "recognize  ----> recognize\n",
            "and  ----> and\n",
            "understand  ----> understand\n",
            "\n",
            "   ----> \n",
            " \n",
            "accurately  ----> accurately\n",
            "from  ----> from\n",
            "the  ----> the\n",
            "start  ----> start\n",
            "if  ----> if\n",
            "those  ----> those\n",
            "applications  ----> application\n",
            "are  ----> be\n",
            "going  ----> go\n",
            "to  ----> to\n",
            "be  ----> be\n",
            "\n",
            "   ----> \n",
            " \n",
            "useful  ----> useful\n",
            ".  ----> .\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text)\n",
        "for i in doc:\n",
        "  print(i, ' ----> '+ i.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enumJWUCppWz"
      },
      "source": [
        "### Wordnet Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UyjenO-ppWz",
        "outputId": "7bfbc2fd-530b-4daa-bc6d-8da70a32e38f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/student/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natural  ----> Natural\n",
            "language  ----> language\n",
            "processing  ----> processing\n",
            "refers  ----> refers\n",
            "to  ----> to\n",
            "the  ----> the\n",
            "branch  ----> branch\n",
            "of  ----> of\n",
            "computer  ----> computer\n",
            "science  ----> science\n",
            "and  ----> and\n",
            "more  ----> more\n",
            "specifically  ----> specifically\n",
            "the  ----> the\n",
            "branch  ----> branch\n",
            "of  ----> of\n",
            "artificial  ----> artificial\n",
            "intelligence  ----> intelligence\n",
            ",  ----> ,\n",
            "concerned  ----> concerned\n",
            "with  ----> with\n",
            "giving  ----> giving\n",
            "computers  ----> computer\n",
            "the  ----> the\n",
            "ability  ----> ability\n",
            "to  ----> to\n",
            "understand  ----> understand\n",
            "text  ----> text\n",
            "and  ----> and\n",
            "spoken  ----> spoken\n",
            "words  ----> word\n",
            "in  ----> in\n",
            "much  ----> much\n",
            "the  ----> the\n",
            "same  ----> same\n",
            "way  ----> way\n",
            "human  ----> human\n",
            "beings  ----> being\n",
            "can  ----> can\n",
            ".  ----> .\n",
            "Human  ----> Human\n",
            "language  ----> language\n",
            "is  ----> is\n",
            "filled  ----> filled\n",
            "with  ----> with\n",
            "ambiguities  ----> ambiguity\n",
            "that  ----> that\n",
            "make  ----> make\n",
            "it  ----> it\n",
            "incredibly  ----> incredibly\n",
            "difficult  ----> difficult\n",
            "to  ----> to\n",
            "write  ----> write\n",
            "software  ----> software\n",
            "that  ----> that\n",
            "accurately  ----> accurately\n",
            "determines  ----> determines\n",
            "the  ----> the\n",
            "intended  ----> intended\n",
            "meaning  ----> meaning\n",
            "of  ----> of\n",
            "text  ----> text\n",
            "or  ----> or\n",
            "voice  ----> voice\n",
            "data  ----> data\n",
            ".  ----> .\n",
            "Homonyms  ----> Homonyms\n",
            ",  ----> ,\n",
            "homophones  ----> homophone\n",
            ",  ----> ,\n",
            "sarcasm  ----> sarcasm\n",
            ",  ----> ,\n",
            "idioms  ----> idiom\n",
            ",  ----> ,\n",
            "metaphors  ----> metaphor\n",
            ",  ----> ,\n",
            "grammar  ----> grammar\n",
            "and  ----> and\n",
            "usage  ----> usage\n",
            "exceptions  ----> exception\n",
            ",  ----> ,\n",
            "variations  ----> variation\n",
            "in  ----> in\n",
            "sentence  ----> sentence\n",
            "structure  ----> structure\n",
            ".  ----> .\n",
            "These  ----> These\n",
            "just  ----> just\n",
            "a  ----> a\n",
            "few  ----> few\n",
            "of  ----> of\n",
            "the  ----> the\n",
            "irregularities  ----> irregularity\n",
            "of  ----> of\n",
            "human  ----> human\n",
            "language  ----> language\n",
            "that  ----> that\n",
            "take  ----> take\n",
            "humans  ----> human\n",
            "years  ----> year\n",
            "to  ----> to\n",
            "learn  ----> learn\n",
            ",  ----> ,\n",
            "but  ----> but\n",
            "that  ----> that\n",
            "programmers  ----> programmer\n",
            "must  ----> must\n",
            "teach  ----> teach\n",
            "natural  ----> natural\n",
            "language  ----> language\n",
            "driven  ----> driven\n",
            "applications  ----> application\n",
            "to  ----> to\n",
            "recognize  ----> recognize\n",
            "and  ----> and\n",
            "understand  ----> understand\n",
            "accurately  ----> accurately\n",
            "from  ----> from\n",
            "the  ----> the\n",
            "start  ----> start\n",
            "if  ----> if\n",
            "those  ----> those\n",
            "applications  ----> application\n",
            "are  ----> are\n",
            "going  ----> going\n",
            "to  ----> to\n",
            "be  ----> be\n",
            "useful  ----> useful\n",
            ".  ----> .\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lem = WordNetLemmatizer()\n",
        "for t in token:\n",
        "  print(t, ' ----> ' + lem.lemmatize(t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEP2orLcppWz"
      },
      "source": [
        "### Textblob Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnGPKw2ippWz",
        "outputId": "ee2b4054-83ee-43ac-fd9b-711c11d059a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natural  ----> Natural\n",
            "language  ----> language\n",
            "processing  ----> processing\n",
            "refers  ----> refers\n",
            "to  ----> to\n",
            "the  ----> the\n",
            "branch  ----> branch\n",
            "of  ----> of\n",
            "computer  ----> computer\n",
            "science  ----> science\n",
            "and  ----> and\n",
            "more  ----> more\n",
            "specifically  ----> specifically\n",
            "the  ----> the\n",
            "branch  ----> branch\n",
            "of  ----> of\n",
            "artificial  ----> artificial\n",
            "intelligence  ----> intelligence\n",
            "concerned  ----> concerned\n",
            "with  ----> with\n",
            "giving  ----> giving\n",
            "computers  ----> computer\n",
            "the  ----> the\n",
            "ability  ----> ability\n",
            "to  ----> to\n",
            "understand  ----> understand\n",
            "text  ----> text\n",
            "and  ----> and\n",
            "spoken  ----> spoken\n",
            "words  ----> word\n",
            "in  ----> in\n",
            "much  ----> much\n",
            "the  ----> the\n",
            "same  ----> same\n",
            "way  ----> way\n",
            "human  ----> human\n",
            "beings  ----> being\n",
            "can  ----> can\n",
            "Human  ----> Human\n",
            "language  ----> language\n",
            "is  ----> is\n",
            "filled  ----> filled\n",
            "with  ----> with\n",
            "ambiguities  ----> ambiguity\n",
            "that  ----> that\n",
            "make  ----> make\n",
            "it  ----> it\n",
            "incredibly  ----> incredibly\n",
            "difficult  ----> difficult\n",
            "to  ----> to\n",
            "write  ----> write\n",
            "software  ----> software\n",
            "that  ----> that\n",
            "accurately  ----> accurately\n",
            "determines  ----> determines\n",
            "the  ----> the\n",
            "intended  ----> intended\n",
            "meaning  ----> meaning\n",
            "of  ----> of\n",
            "text  ----> text\n",
            "or  ----> or\n",
            "voice  ----> voice\n",
            "data  ----> data\n",
            "Homonyms  ----> Homonyms\n",
            "homophones  ----> homophone\n",
            "sarcasm  ----> sarcasm\n",
            "idioms  ----> idiom\n",
            "metaphors  ----> metaphor\n",
            "grammar  ----> grammar\n",
            "and  ----> and\n",
            "usage  ----> usage\n",
            "exceptions  ----> exception\n",
            "variations  ----> variation\n",
            "in  ----> in\n",
            "sentence  ----> sentence\n",
            "structure  ----> structure\n",
            "These  ----> These\n",
            "just  ----> just\n",
            "a  ----> a\n",
            "few  ----> few\n",
            "of  ----> of\n",
            "the  ----> the\n",
            "irregularities  ----> irregularity\n",
            "of  ----> of\n",
            "human  ----> human\n",
            "language  ----> language\n",
            "that  ----> that\n",
            "take  ----> take\n",
            "humans  ----> human\n",
            "years  ----> year\n",
            "to  ----> to\n",
            "learn  ----> learn\n",
            "but  ----> but\n",
            "that  ----> that\n",
            "programmers  ----> programmer\n",
            "must  ----> must\n",
            "teach  ----> teach\n",
            "natural  ----> natural\n",
            "language  ----> language\n",
            "driven  ----> driven\n",
            "applications  ----> application\n",
            "to  ----> to\n",
            "recognize  ----> recognize\n",
            "and  ----> and\n",
            "understand  ----> understand\n",
            "accurately  ----> accurately\n",
            "from  ----> from\n",
            "the  ----> the\n",
            "start  ----> start\n",
            "if  ----> if\n",
            "those  ----> those\n",
            "applications  ----> application\n",
            "are  ----> are\n",
            "going  ----> going\n",
            "to  ----> to\n",
            "be  ----> be\n",
            "useful  ----> useful\n"
          ]
        }
      ],
      "source": [
        "#!pip install textblob\n",
        "from textblob import TextBlob\n",
        "blob = TextBlob(text)\n",
        "for i in blob.words:\n",
        "  print(i, ' ----> ' + i.lemmatize())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwGFx_UJppWz"
      },
      "source": [
        "## Pattern Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MVr_EhNyppWz",
        "outputId": "9a2229e3-6b5f-41a1-df80-34f9de554ba9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the bat see the cat with best stripe hang upside down by their feet\n",
            "[['the', 'thes', 'thing', 'thed'], ['bat', 'bats', 'batting', 'batted'], ['see', 'sees', 'seeing', 'saw', 'seen'], ['the', 'thes', 'thing', 'thed'], ['cat', 'cats', 'catting', 'catted'], ['with', 'withs', 'withing', 'withed'], ['best', 'bests', 'besting', 'bested'], ['stripe', 'stripes', 'striping', 'striped'], ['hang', 'hangs', 'hanging', 'hung'], ['upside', 'upsides', 'upsiding', 'upsided'], ['down', 'downs', 'downing', 'downed'], ['by', 'bies', 'bying', 'bied'], ['their', 'theirs', 'theiring', 'theired'], ['feet', 'feets', 'feeting', 'feeted']]\n"
          ]
        }
      ],
      "source": [
        "from pattern.en import lemma, lexeme, parse\n",
        "sentence = \"the bats saw the cats with best stripes hanging upside down by their feet\"\n",
        "lemmatized_sentence = \" \".join([lemma(word) for word in sentence.split()])\n",
        "print(lemmatized_sentence)\n",
        "\n",
        "#Special Feature : to get all possible lemmas for each word in the sentence\n",
        "all_lemmas_for_each_word = [lexeme(wd) for wd in sentence.split()]\n",
        "print(all_lemmas_for_each_word)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VssvPHJSpzog"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.18 ('AML_NNM22AM010')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "61eebffdc315091c50fb0c840f26aabfb1342648934348f4862a0a23133400a5"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}